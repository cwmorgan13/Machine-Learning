import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
basedir = os.path.join(r'\\frascc1','sku_cntrl','Margin','ChrisMorgan','titanic')

train = pd.read_csv(os.path.join(basedir,'train.csv'))

from sklearn.preprocessing import LabelEncoder
enc = LabelEncoder()

train['Cabin'] = train['Cabin'].fillna('None')
train['Embarked'] = train['Embarked'].fillna('N')
train['Age'] = train['Age'].fillna(train['Age'].mean())

col = ['Name','Sex','Ticket','Cabin','Embarked']
for col in ['Name','Sex','Ticket','Cabin','Embarked']:
    train[col + '_code'] = enc.fit_transform(train[col])

trainset = train.drop(['PassengerId','Name','Sex','Ticket','Cabin','Embarked'], axis = 1)

x = trainset.drop(['Survived'], axis = 1)
y = trainset[['Survived']]

x['Bias'] = 1

x = x.values

y = y.values

def Sigmoid(z):
    d = 1/(1+np.exp(-z))
    return d

theta = np.random.random((x.shape[1],1))

def CostFunction(x,y,theta):
    m = len(y)
    prediction = np.dot(x,theta)
    sigmoid = Sigmoid(prediction)
    error1 = (y.T.dot(np.log(sigmoid)))
    error2 = (((1-y).T).dot((np.log(1-sigmoid))))
    error = -sum(error1+error2)
    J = (1/m)*(error)
    return J
    
def LogGradientDescent(x,y,theta,lr,its):
    m = len(y)
    thetah = np.zeros((its,len(theta)))
    costh = np.zeros(its)
    for it in range(its):
        pred = x.dot(theta)
        grad = ((lr/m) * (x.T.dot(Sigmoid(pred) - y)))
        theta = theta - grad
        thetah[it,:] = theta.T
        costh[it] = CostFunction(x,y,theta)
    return theta, costh, thetah
        
def Prediction(x,theta):
    p = np.round(Sigmoid(x.dot(theta)))
    return p
    
def fit(x,y,lr = 0.01,its = 100):
    fit = LogGradientDescent(x,y,theta,lr,its)
    return fit

fit = fit(x,y,lr = 0.01,its = 5000)

pred = Prediction(x,theta)

plt.plot(pred)
